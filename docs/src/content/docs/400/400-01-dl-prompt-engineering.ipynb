{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 400.1 | Prompt Engineering For Developers\n",
    "\n",
    "Notes and exercises based on the [ChatGPT Prompt Engineering For Developers](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/) course from DeepLearning.AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 0. Validate Setup\n",
    "\n",
    "Before you begin working on exercises in this section, make sure you are setup to work with the right LLM Providers by running the relevant code cells from the [LLM Setup](./400-00-llm-setup.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. Introduction \n",
    "\n",
    "This course was created when OpenAI was using version 0.28. In this notebook I hope to use the course for insights, but use it in conjunction with the OpenAI documentation to update the learnings for the latest OpenAI versions and practices. Quick study notes\n",
    "\n",
    "1. LLM = Large Language Model. Understands Natural Language. Takes NLP inputs (prompts), returns NLP outputs (responses).\n",
    "1. 2 types of LLM = Base LLM (predicts next word), Instruction Tuned LLM (follows instructions)\n",
    "1. Base LLM = Default, also referred to as Foundation Models that are trained on large corpora of text data.\n",
    "1. Instruction Tuned LLM = Fine-tuned version of base LLM using RLHF (Reinforcement Learning from Human Feedback) to follow instructions.\n",
    "1. Generative AI Focus = Increasingly on Instruction Tuned LLMs, and using NLP to have AI execute tasks based on instruction prompts.\n",
    "\n",
    "LLMs are smart in the sense of having a massive knowledge base and fast pattern-matching capabilities. But they lack the human cognitive ability to understand context, reason, and infer meaning. Instead, they are effectively reflecting the data they were trained on - which means instruction-tuned LLMs are only as effective as the clarity and specificity of instructions they are given.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. Guidelines for Prompting\n",
    "\n",
    "The chaper focuses on 2 main guidelines:\n",
    "1. Write clear and concise instructions\n",
    "1. Give the model time to think.\n",
    "\n",
    "The course has its own sandbox environment for practice, but I want to try my own experiments here. \n",
    "- Refer to the [Setup notebook](400-00-llm-setup.ipynb) for code snippets for chat completion\n",
    "- Refer to the [OpenAI API documentation](https://platform.openai.com/docs/quickstart?context=python) for quickstarts\n",
    "- Refer to the [OpenAI Cookbook](https://cookbook.openai.com/) for richer examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an OpenAI client instance\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# Create a helper function to reuse across exercises\n",
    "# So we can focus on prompt engineering more clearly\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "  messages = [{\"role\": \"user\", \"content\": prompt }]\n",
    "  completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    temperature=0, #degree of randomness\n",
    "  )\n",
    "  return completion.choices[0].message.content\n",
    "\n",
    "# Prompting Principles\n",
    "# 1. Write clear and specific instructions\n",
    "# 2. Give the model time to \"think\"\n",
    "# ====================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tactic: Use Delimiters to Guide Model\n",
    "# =====================================\n",
    "\n",
    "# Make sure cells are run in order to have client setup.\n",
    "\n",
    "text = f\"\"\"\n",
    "You should express what you want a model to do by \\ \n",
    "providing instructions that are as clear and \\ \n",
    "specific as you can possibly make them. \\ \n",
    "This will guide the model towards the desired output, \\ \n",
    "and reduce the chances of receiving irrelevant \\ \n",
    "or incorrect responses. Don't confuse writing a \\ \n",
    "clear prompt with writing a short prompt. \\ \n",
    "In many cases, longer prompts provide more clarity \\ \n",
    "and context for the model, which can lead to \\ \n",
    "more detailed and relevant outputs.\n",
    "\"\"\"\n",
    "\n",
    "# Craft Your Prompt\n",
    "prompt = f\"\"\"\n",
    "Summarize the text delimited by triple backticks \\ \n",
    "into a single sentence.\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "# Print Completion Response\n",
    "print(get_completion(prompt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
