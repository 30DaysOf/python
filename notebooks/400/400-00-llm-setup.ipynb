{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup & Verify SDKs\n",
    "\n",
    "The exercises in this section will have dependencies on various Large Language Model providers. Run this notebook to make sure all your environment variables are setup and the SDKs are installed correctly. Refresh code segments from relevant LLM quickstarts periodically to ensure you are working with the latest APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. With Open AI\n",
    "\n",
    "We should use the [latest documentation](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/) and API (v1.0+) to validate our learnings. Here's the [quickstart](https://platform.openai.com/docs/quickstart?context=python) steps and samples we can use to validate this for our setup\n",
    "\n",
    "1. **Setup Python Environment** - I already have this setup with the Dev Container configuration, using Python 3.\n",
    "2. **Install OpenAI Python Package** - Added to default `requirements.txt`. We can upgrade manually with `pip install --upgrade openai`.\n",
    "3. **Configure API Key** - Create an OpenAI account, get API key - set that as environment variable `OPENAI_API_KEY` in .env \n",
    "4. **Validate Usage** - Run the code cell below to verify `Chat Completions`, `Embeddings` and `Images` capabilities work by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chat Completions API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There once was a function so sly,\n",
      "In itself, it did love to lie.\n",
      "With a call to its name,\n",
      "It played the recursion game,\n",
      "Continuing on 'til the stack said goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Create an OpenAI client instance\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# Verify Chat Completions API works\n",
    "# https://platform.openai.com/docs/guides/text-generation/chat-completions-api\n",
    "#\n",
    "# Chat models take a list of messages as input and return a model-generated message as output. \n",
    "# It's designed to make multi-turn conversations easy but also works for single-turn tasks with no conversation.\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Compose a limerick that explains the concept of recursion in programming\"}\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chat Completions API (Streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, say can you see, by the dawn's early light,\n",
      "What so proudly we hailed at the twilight's last gleaming,\n",
      "Whose broad stripes and bright stars through the perilous fight,\n",
      "O'er the ramparts we watched, were so gallantly streaming?\n",
      "And the rocket's red glare, the bombs bursting in air,\n",
      "Gave proof through the night that our flag was still there.\n",
      "O say does that star-spangled banner yet wave\n",
      "O’er the land of the free and the home of the brave?"
     ]
    }
   ],
   "source": [
    "# Create an OpenAI client instance\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# Verify Chat Completion AI works with STREAMING\n",
    "# https://platform.openai.com/docs/api-reference/streaming\n",
    "#\n",
    "# This allows the server to stream responses back to the client as they are generated, \n",
    "# allowing clients to show partial results for certain requests\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Send the lyrics to the US National Anthem one sentence at a time\"}],\n",
    "    stream=True,\n",
    ")\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assistants API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Steps:\n",
      "\n",
      "Step Type: message_creation\n",
      "Total Tokens Used: 304\n",
      "\n",
      "Step Type: tool_calls\n",
      "Total Tokens Used: 252\n",
      "\n",
      "Step Type: message_creation\n",
      "Total Tokens Used: 189\n",
      "\n",
      "\n",
      "6. Messages:\n",
      "\n",
      "Role: assistant\n",
      "Value: The solution to the equation \\(3x + 11 = 14\\) is \\(x = 1\\). If you have any more questions or need further assistance, feel free to ask!\n",
      "\n",
      "Role: assistant\n",
      "Value: Sure, Jane! To solve the equation \\(3x + 11 = 14\\), we need to isolate the variable \\(x\\) on one side of the equation. I will show you step by step how to do that. Let's begin.\n",
      "\n",
      "Role: user\n",
      "Value: I need to solve the equation `3x + 11 = 14`. Can you help me?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an OpenAI client instance\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "  \n",
    "# Verify Assistants API \n",
    "# https://platform.openai.com/docs/assistants/overview?context=without-streaming\n",
    "#\n",
    "# Build your own Assistant (chat function with deployed endpoint) to respond to user queries\n",
    "# Leverage models, tools and knowledge for creating effective responses.\n",
    "# 3 Tool Types Available: Code Interpreter, Retrieval and Function calling.\n",
    "\n",
    "# Step 1: Create an Assistant\n",
    "assistant = client.beta.assistants.create(\n",
    "  name=\"Math Tutor\",\n",
    "  instructions=\"You are a personal math tutor. Write and run code to answer math questions.\",\n",
    "  tools=[{\"type\": \"code_interpreter\"}],\n",
    "  model=\"gpt-4-turbo-preview\",\n",
    ")\n",
    "\n",
    "# Step 2: Create a Thread\n",
    "thread = client.beta.threads.create()\n",
    "\n",
    "# Step 3: Add a Message to the Thread\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"I need to solve the equation `3x + 11 = 14`. Can you help me?\"\n",
    ")\n",
    "\n",
    "# Step 4: Create a Run (Non-Streaming)\n",
    "run = client.beta.threads.runs.create_and_poll(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id,\n",
    "  instructions=\"Please address the user as Jane Doe. The user has a premium account.\"\n",
    ")\n",
    "\n",
    "# Step 5 (Optional): List Run steps in completed Run\n",
    "if run.status == 'completed': \n",
    "  steps = client.beta.threads.runs.steps.list(\n",
    "    thread_id=thread.id,\n",
    "    run_id=run.id\n",
    "  )\n",
    "  print(\"\\n5. Steps:\\n\")\n",
    "  for step in steps.data:\n",
    "      print(\"Step Type:\", step.step_details.type)\n",
    "      print(\"Total Tokens Used:\", step.usage.total_tokens)\n",
    "      print()\n",
    "else:\n",
    "  print(\"\\n5. Status:\\n\", run.status)\n",
    "\n",
    "# Step 6 (Optional): List Messages in completed Run\n",
    "if run.status == 'completed': \n",
    "  messages = client.beta.threads.messages.list(\n",
    "    thread_id=thread.id\n",
    "  )\n",
    "  print(\"\\n6. Messages:\\n\")\n",
    "  for message in messages.data:\n",
    "      print(\"Role:\", message.role)\n",
    "      print(\"Value:\", message.content[0].text.value)\n",
    "      print()\n",
    "else:\n",
    "  print(\"\\n6. Status:\\n\",run.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assistants API (Streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "assistant > Sure! To solve the equation \\(3x + 11 = 14\\) for \\(x\\), we first need to isolate \\(x\\) on one side of the equation. This can be done by following these steps:\n",
      "\n",
      "1. Subtract \\(11\\) from both sides of the equation to eliminate the \\(+11\\) on the left side.\n",
      "2. Divide both sides of the equation by \\(3\\) to solve for \\(x\\).\n",
      "\n",
      "Let's calculate the value of \\(x\\).\n",
      "assistant > code_interpreter\n",
      "\n",
      "from sympy import symbols, Eq, solve\n",
      "\n",
      "# Define the variable\n",
      "x = symbols('x')\n",
      "\n",
      "# Define the equation\n",
      "equation = Eq(3*x + 11, 14)\n",
      "\n",
      "# Solve the equation for x\n",
      "solution = solve(equation, x)\n",
      "solution\n",
      "\n",
      "output >\n",
      "\n",
      "[1]\n",
      "\n",
      "assistant > The solution to the equation \\(3x + 11 = 14\\) is \\(x = 1\\)."
     ]
    }
   ],
   "source": [
    "# Create an OpenAI client instance\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "  \n",
    "# Verify Assistants API \n",
    "# https://platform.openai.com/docs/assistants/overview?context=without-streaming\n",
    "#\n",
    "# Build your own Assistant (chat function with deployed endpoint) to respond to user queries\n",
    "# Leverage models, tools and knowledge for creating effective responses.\n",
    "# 3 Tool Types Available: Code Interpreter, Retrieval and Function calling.\n",
    "\n",
    "# Step 1: Create an Assistant\n",
    "assistant = client.beta.assistants.create(\n",
    "  name=\"Math Tutor\",\n",
    "  instructions=\"You are a personal math tutor. Write and run code to answer math questions.\",\n",
    "  tools=[{\"type\": \"code_interpreter\"}],\n",
    "  model=\"gpt-4-turbo-preview\",\n",
    ")\n",
    "\n",
    "# Step 2: Create a Thread\n",
    "thread = client.beta.threads.create()\n",
    "\n",
    "# Step 3: Add a Message to the Thread\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"I need to solve the equation `3x + 11 = 14`. Can you help me?\"\n",
    ")\n",
    "# Step 4: Create a Run (Streaming)\n",
    "from typing_extensions import override\n",
    "from openai import AssistantEventHandler\n",
    " \n",
    "# First, we create a EventHandler class to define\n",
    "# how we want to handle the events in the response stream.\n",
    " \n",
    "class EventHandler(AssistantEventHandler):    \n",
    "  @override\n",
    "  def on_text_created(self, text) -> None:\n",
    "    print(f\"\\nassistant > \", end=\"\", flush=True)\n",
    "      \n",
    "  @override\n",
    "  def on_text_delta(self, delta, snapshot):\n",
    "    print(delta.value, end=\"\", flush=True)\n",
    "      \n",
    "  def on_tool_call_created(self, tool_call):\n",
    "    print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n",
    "  \n",
    "  def on_tool_call_delta(self, delta, snapshot):\n",
    "    if delta.type == 'code_interpreter':\n",
    "      if delta.code_interpreter.input:\n",
    "        print(delta.code_interpreter.input, end=\"\", flush=True)\n",
    "      if delta.code_interpreter.outputs:\n",
    "        print(f\"\\n\\noutput >\", flush=True)\n",
    "        for output in delta.code_interpreter.outputs:\n",
    "          if output.type == \"logs\":\n",
    "            print(f\"\\n{output.logs}\", flush=True)\n",
    " \n",
    "# Then, we use the `create_and_stream` SDK helper \n",
    "# with the `EventHandler` class to create the Run \n",
    "# and stream the response.\n",
    " \n",
    "with client.beta.threads.runs.stream(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id,\n",
    "  instructions=\"Please address the user as Jane Doe. The user has a premium account.\",\n",
    "  event_handler=EventHandler(),\n",
    ") as stream:\n",
    "  stream.until_done()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. With Azure Open AI\n",
    "\n",
    "We should use the [latest documentation]() and [Quickstarts](https://learn.microsoft.com/en-us/azure/ai-services/openai/quickstart?tabs=command-line%2Cpython-new&pivots=programming-language-python) to validate our learnings. Here are the common quickstart steps:\n",
    "\n",
    "1. **Setup Python Environment** - I already have this setup with the Dev Container configuration, using Python 3.\n",
    "2. **Install OpenAI Python Package** - Added to default `requirements.txt`. We can upgrade manually with `pip install --upgrade openai`.\n",
    "3. **Configure API Key** - Create an Azure OpenAI resource, [visit Azure Portal](https://learn.microsoft.com/en-us/azure/ai-services/openai/quickstart?tabs=command-line%2Cpython-new&pivots=programming-language-python#retrieve-key-and-endpoint) and retrieve Endpoint, API-Key and Deployment Name\n",
    "4. **Update Environment Variables** - Set the \"AZURE_\" prefixed variables  in `.env` file (see `.env.sample` for reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Completion API (Single Question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "Write a tagline for the March Madness games. \n",
      "Completion:\n",
      " Choose one or make your own. 23 is number 1! March Madness, who's basketballs will drop first?\n"
     ]
    }
   ],
   "source": [
    "# Create an Azure OpenAI client instance\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),  \n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "\n",
    "# Verify Chat Completions API works\n",
    "# https://learn.microsoft.com/en-us/azure/ai-services/openai/quickstart?tabs=command-line%2Cpython-new&pivots=programming-language-python#create-a-new-python-application\n",
    "\n",
    "# Try a single-message completion task\n",
    "start_phrase = 'Write a tagline for the March Madness games. '\n",
    "completion = client.completions.create(\n",
    "    model=deployment_name, \n",
    "    prompt=start_phrase, \n",
    "    max_tokens=25\n",
    ")\n",
    "print('Prompt:\\n' + start_phrase)\n",
    "print(\"Completion:\\n\"+ completion.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Completion API (Multi-Turn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There once was a coder named Larry,\n",
      "Whose problem was getting quite hairy,\n",
      "But with recursion in sight,\n",
      "He found the way to write\n",
      "A function that called itself, oh so merry!\n",
      "\n",
      "And so the program worked like a charm,\n",
      "Through iterations and loops, no alarm,\n",
      "For recursion, you see,\n",
      "Is a powerful key\n",
      "To solving problems with code's magic arm.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "  api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "  api_version=\"2024-02-01\"\n",
    ")\n",
    "deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model= deployment_name, #gpt-35-turbo\", # model = \"deployment_name\".\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"},\n",
    "      {\"role\": \"user\", \"content\": \"Compose a limerick that explains the concept of recursion in programming\"}\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Hugging Face\n",
    "\n",
    "We should use the [latest documentation](https://huggingface.co/docs) and focus initially on the [Text Generation Inference](https://huggingface.co/docs/text-generation-inference/index) capability. We can use this in two ways:\n",
    " 1. Use the [Inference Client](https://huggingface.co/docs/text-generation-inference/basic_tutorials/consuming_tgi#inference-client) to talk to a deployed model directly.\n",
    " 1. Use the [Messages API](https://huggingface.co/docs/text-generation-inference/messages_api) to interact with deployed model using API compatible with OpenAI Chat Completions API\n",
    "\n",
    "Note: [Text Generation Inference](https://github.com/huggingface/text-generation-inference) is a [Hugging Face toolkit](https://huggingface.co/docs/text-generation-inference/index) for deploying and serving Large Language Models (LLM) - working well with popular open-source LLMs including LLama, Falcom, StarCoder, etc. To use the client above, we need a running instance of the Inference API. There are a few options:\n",
    "1. Run a local TGI server. Use the [Docker](https://huggingface.co/docs/text-generation-inference/quicktour) quickstart, or [install locally from source](https://huggingface.co/docs/text-generation-inference/installation).\n",
    "1. Explore it using the [Hugging Chat](https://huggingface.co/chat) UI which works against their production TGI deployment endpoint.\n",
    "1. Explore hosting your own service in the cloud - for example [using Azure Container Instances](https://medium.com/thedeephub/deploy-hugging-face-text-generation-inference-on-azure-container-instance-3709eb3d3187).\n",
    "1. Use the free (but rate-limited) [Serverless Inference API](https://huggingface.co/docs/api-inference/index) endpoint provided by Hugging Face for testing and evaluation purposes - works with models in hub. \n",
    "1. Use their production [Inference Endpoints Service](https://huggingface.co/docs/inference-endpoints/index) paid service - and bring your own model deployment (e.g., using Azure Container Registry).\n",
    "\n",
    "Running locally works only if you have [supported hardware](https://huggingface.co/docs/text-generation-inference/supported_models#supported-hardware). Given that, our best option is to explore ideas using the [Free Serverless Inference API](https://huggingface.co/docs/api-inference/index) for now. Here are the [quickstart](https://huggingface.co/docs/api-inference/quicktour) steps:\n",
    "\n",
    "1. **[Get an API Token](https://huggingface.co/docs/api-inference/quicktour#get-your-api-token)**: Sign up for a free account to get one.\n",
    "1. **[Select your Model](https://huggingface.co/docs/api-inference/quicktour#running-inference-with-api-requests)**: Pick one from the Hub and custmize your URL.\n",
    "1. **[Run Inference on Endpoint](https://huggingface.co/docs/api-inference/quicktour#running-inference-with-api-requests)**: Invoke REST API and use [detailed parameters](https://huggingface.co/docs/api-inference/detailed_parameters) as needed for task.\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic TGI Inference - Text query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      " My name is Betty Crocker and I am   a doctor. Not only did the doctor give me an epidural but also an epidural to make me a better husband and father to my daughter after an affair went wrong. He even had\n"
     ]
    }
   ],
   "source": [
    "# The Hosted API Inference endpoint allows you to interact with deployed models from the Hub.\n",
    "# The basic inference endpoint allows you to interact with models using a simple text input.\n",
    "# The advanced inference endpoint allows you to interact with models using detailed parameters that map to a Task.\n",
    "\n",
    "# Let's try the basic inference first.\n",
    "# Visit the Model Hub: https://huggingface.co/models\n",
    "# Select a model and visit its Model Card: https://huggingface.co/openai-community/gpt2\n",
    "# Verify that it supports the **Inference API** (see card right)\n",
    "# Invoke the model with a simple string input (may not work for all)\n",
    "import os\n",
    "apikey=os.getenv(\"HUGGING_FACE_API_KEY\")\n",
    "#model=os.getenv(\"HUGGING_FACE_MODEL\")\n",
    "model=\"openai-community/gpt2\"\n",
    "\n",
    "import requests\n",
    "model_ep = f\"https://api-inference.huggingface.co/models/{model}\"\n",
    "headers = {\"Authorization\": f\"Bearer {apikey}\"}\n",
    "def query(payload):\n",
    "    response = requests.post(model_ep, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "# Formulate query for model specific needs\n",
    "data = query(\"My name is Betty Crocker and I am  \")\n",
    "print(\"Response:\\n \"+ data[0].get(\"generated_text\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task-Specific TGI Inference - Use Parameters\n",
    "\n",
    "If you now replace the model with a different model like \"mistralai/Mistral-7B-Instruct-v0.2\" and run the same code, you will likely see errors - even though the model card tells us it supports the Inference API. This is because the model may require _detailed parameters_ for use with the default task we are trying to achieve. For example, we can see that this model card indicates it is useful for Text Generation tasks - and from [the Text Generation Task](https://huggingface.co/docs/api-inference/detailed_parameters#text-generation-task) definition, we see it requires a mandataory **inputs** parameter. Let's try creating these \"task-specific\" requests next. You can find detailed paramters for the following tasks at their respective links.\n",
    "\n",
    "1. **Natural Language Processing** 👇🏽 <br/> [Fill Mask task](https://huggingface.co/docs/api-inference/detailed_parameters#fill-mask-task) -- [Summarization Task](https://huggingface.co/docs/api-inference/detailed_parameters#summarization-task) -- [Question Answering Task](https://huggingface.co/docs/api-inference/detailed_parameters#question-answering-task) --- [Table Question Answering Task](https://huggingface.co/docs/api-inference/detailed_parameters#table-question-answering-task) --- [Sentence Similarity Task](https://huggingface.co/docs/api-inference/detailed_parameters#sentence-similarity-task) --- [Text Classification Task](https://huggingface.co/docs/api-inference/detailed_parameters#text-classification-task) --- [Text Generation Task](https://huggingface.co/docs/api-inference/detailed_parameters#text-generation-task) --- [Text2Text Generation Task](https://huggingface.co/docs/api-inference/detailed_parameters#text2text-generation-task) --- [Token Classification Task](https://huggingface.co/docs/api-inference/detailed_parameters#token-classification-task) --- [Named Entity Recognition Task (NER)](https://huggingface.co/docs/api-inference/detailed_parameters#named-entity-recognition-ner-task) --- [Translation Task](https://huggingface.co/docs/api-inference/detailed_parameters#translation-task) --- [Zero-Shot Classification Task](https://huggingface.co/docs/api-inference/detailed_parameters#zero-shot-classification-task) --- [Conversational Task](https://huggingface.co/docs/api-inference/detailed_parameters#conversational-task) --- [Feature Extraction Task](https://huggingface.co/docs/api-inference/detailed_parameters#feature-extraction-task) \n",
    "2. **Audio** 👇🏽 <br/> [Audio Classification Task](https://huggingface.co/docs/api-inference/detailed_parameters#audio-classification-task) --- [Automatic Speech Recognition Task](https://huggingface.co/docs/api-inference/detailed_parameters#automatic-speech-recognition-task) \n",
    "3. **Computer Vision** 👇🏽 <br/> [Image Classification Task](https://huggingface.co/docs/api-inference/detailed_parameters#image-classification-task) --- [Object Detection Task](https://huggingface.co/docs/api-inference/detailed_parameters#object-detection-task) --- [Image Segmentation Task](https://huggingface.co/docs/api-inference/detailed_parameters#image-segmentation-task)\n",
    "\n",
    "In the section below, we can look at a few examples - and we can refer to the docs to replicate the usage for others. By default each documented task also provides a _recommended model_ for you to use. However, I also find it useful to visit the [Model Hub](https://huggingface.co/models) and select the task category (at left) and pick a new or trending model (at right) to try out with the task parameters.\n",
    "\n",
    "🚨 **WARNING** | Hugging Face is Model Hub with many trending and less-popular models. \n",
    " - When you look up a model card (e.g., https://huggingface.co/deepset/roberta-base-squad2 ) you may see a note indicating _This model can belkoaded on Inference API (serverless)_ \n",
    " - **but** also see an alert below that says **Model not loaded yet**. This means that the model may get loaded on demand \n",
    " - so the first serverless invocation may fail as the request triggers the initial load. Subsquent requests should pass. \n",
    " - _The returned response may say something like this_: ``` {'error': 'Model deepset/roberta-base-squad2 is currently loading', 'estimated_time': 20.0}```\n",
    " - Use this information at the client to make sure you wait-and-retry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your API Key and Model\n",
    "# Verify that the MODEL you selected shows the Summarization TASK in its model card."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP: Text Generation LLM (Mistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "  [{'generated_text': 'The answer to the universe is 42: a popular and iconic quote that quenches the enormous curiosity of millions, possibly billions, of viewers of science-fiction teaser, The Hitchhiker’s Guide to the Galaxy, written by Douglas Adams. A question from a very young friend made me ponder: what inspired scholars to come up with a number to answer the universe’s riddle? My best guess is that this nonsensical quote fills an emptiness for many.\\n'}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "apikey=os.getenv(\"HUGGING_FACE_API_KEY\")\n",
    "model=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "import requests\n",
    "model_ep = f\"https://api-inference.huggingface.co/models/{model}\"\n",
    "headers = {\"Authorization\": f\"Bearer {apikey}\"}\n",
    "def query(payload):\n",
    "    response = requests.post(model_ep, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "# Text Generation Payload\n",
    "data = query({\n",
    "    \"inputs\": \"The answer to the universe is\"\n",
    "})\n",
    "print(\"Response:\\n \", data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP: Text Generation SLM (phi-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "  {'error': 'Model microsoft/phi-2 is currently loading', 'estimated_time': 222.3747100830078}\n"
     ]
    }
   ],
   "source": [
    "# Model Loading Estimate\n",
    "#   {'error': 'Model microsoft/phi-2 is currently loading', 'estimated_time': 222.3747100830078}\n",
    "import os\n",
    "apikey=os.getenv(\"HUGGING_FACE_API_KEY\")\n",
    "model=\"microsoft/phi-2\"\n",
    "\n",
    "import requests\n",
    "model_ep = f\"https://api-inference.huggingface.co/models/{model}\"\n",
    "headers = {\"Authorization\": f\"Bearer {apikey}\"}\n",
    "def query(payload):\n",
    "    response = requests.post(model_ep, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "# Text Generation Payload\n",
    "data = query(\"Write a detailed analogy between mathematics and a lighthouse.\")\n",
    "print(\"Response:\\n \", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP: Summarization (bart-large-CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "  [{'summary_text': 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world.'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "apikey=os.getenv(\"HUGGING_FACE_API_KEY\")\n",
    "model=\"facebook/bart-large-cnn\"\n",
    "\n",
    "import requests\n",
    "model_ep = f\"https://api-inference.huggingface.co/models/{model}\"\n",
    "headers = {\"Authorization\": f\"Bearer {apikey}\"}\n",
    "def query(payload):\n",
    "    response = requests.post(model_ep, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "# Text Summarization Payload\n",
    "data = query(\n",
    "    {\n",
    "        \"inputs\": \"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\",\n",
    "        \"parameters\": {\"do_sample\": False},\n",
    "    }\n",
    ")\n",
    "print(\"Response:\\n \", data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP: Question Answering (roberta-base-squad2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "  {'score': 0.9151504039764404, 'start': 35, 'end': 41, 'answer': 'Berlin'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "apikey=os.getenv(\"HUGGING_FACE_API_KEY\")\n",
    "model=\"deepset/roberta-base-squad2\"\n",
    "\n",
    "import requests\n",
    "model_ep = f\"https://api-inference.huggingface.co/models/{model}\"\n",
    "headers = {\"Authorization\": f\"Bearer {apikey}\"}\n",
    "def query(payload):\n",
    "    response = requests.post(model_ep, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "# Question Answering Payload\n",
    "data = query(\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"question\": \"Where do I live?\",\n",
    "            \"context\": \"MMy name is Wolfgang and I live in Berlin\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# This model is often not loaded by default till the first request - so you see this:\n",
    "#  {'error': 'Model deepset/roberta-base-squad2 is currently loading', 'estimated_time': 20.0}\n",
    "# Back off and retry in a while. Result will look something like this:\n",
    "#  {'score': 0.9326565265655518, 'start': 11, 'end': 16, 'answer': 'Clara'}\n",
    "print(\"Response:\\n \", data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP: Sentence Similarity (bge-m3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "  [0.8589119911193848, 0.9666367769241333, 0.7509792447090149]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "apikey=os.getenv(\"HUGGING_FACE_API_KEY\")\n",
    "model=\"BAAI/bge-m3\"\n",
    "\n",
    "import requests\n",
    "model_ep = f\"https://api-inference.huggingface.co/models/{model}\"\n",
    "headers = {\"Authorization\": f\"Bearer {apikey}\"}\n",
    "def query(payload):\n",
    "    response = requests.post(model_ep, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "# Sentence Similarity Payload\n",
    "data = query(\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"source_sentence\": \"That is a happy person\",\n",
    "            \"sentences\": [\"That is a happy dog\", \"That is a very happy person\", \"Today is a sunny day\"],\n",
    "        }\n",
    "    }\n",
    ")\n",
    "print(\"Response:\\n \", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP: Zero-Shot Classification (bart-large-mnli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "  {'sequence': 'Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!', 'labels': ['refund', 'faq', 'legal'], 'scores': [0.8777878284454346, 0.10522636026144028, 0.01698581501841545]}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "apikey=os.getenv(\"HUGGING_FACE_API_KEY\")\n",
    "model=\"facebook/bart-large-mnli\"\n",
    "\n",
    "import requests\n",
    "model_ep = f\"https://api-inference.huggingface.co/models/{model}\"\n",
    "headers = {\"Authorization\": f\"Bearer {apikey}\"}\n",
    "def query(payload):\n",
    "    response = requests.post(model_ep, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "# Zero-Shot Classification Payload\n",
    "data = query(\n",
    "    {\n",
    "        \"inputs\": \"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!\",\n",
    "        \"parameters\": {\"candidate_labels\": [\"refund\", \"legal\", \"faq\"]},\n",
    "    }\n",
    ")\n",
    "print(\"Response:\\n \", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Lamini\n",
    "\n",
    "Lamini is an LLM platform [optimized for enterprise fine tuning](https://lamini-ai.github.io/about/).\n",
    " - Explore the [Lamini SDK](https://github.com/lamini-ai/lamini-sdk/)\n",
    " - Explore tools for [better inference](https://lamini-ai.github.io/inference/quick_tour/) \n",
    " - Exolore tools for [better training](https://lamini-ai.github.io/training/quick_tour/)\n",
    "\n",
    "To get started:\n",
    " - Create an account and get an API key\n",
    " - Validate the key works with sample questions as shown\n",
    "\n",
    "Note: The free account only gives you 200 calls _total_ (no refresh) so use it wisely.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the lamini package\n",
    "# !pip install --upgrade lamini\n",
    "\n",
    "## Configure the API key\n",
    "import lamini\n",
    "import os\n",
    "lamini.api_key = os.getenv(\"LAMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15:03:33:11,976 INFO     [lamini.py:33] Using 3.10 InferenceQueue Interface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status code: 513 https://api.lamini.ai/v1/completions\n"
     ]
    },
    {
     "ename": "APIError",
     "evalue": "API error {'detail': \"error_id: 243549526076307102879929981439376352577: Downloading the 'Intel/neural-chat-7b-v3-1' model. Please try again in a few minutes.\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/lamini/api/rest_requests.py:132\u001b[0m, in \u001b[0;36mmake_web_request\u001b[0;34m(key, url, http_method, json)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m     \u001b[43mresp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 513 Server Error:  for url: https://api.lamini.ai/v1/completions",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 22\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m## Option 1: Use a named model to get an endpoint for requests\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m## Models may not be pre-loaded in HF inference service - you will then see this error, so retry:\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m##     Downloading the 'Intel/neural-chat-7b-v3-1' model. \u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m##     Please try again in a few minutes.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m llm \u001b[38;5;241m=\u001b[39m lamini\u001b[38;5;241m.\u001b[39mLamini(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntel/neural-chat-7b-v3-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHow to convert inches to centimeters? Answer in 2 sentences\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/lamini/api/lamini.py:71\u001b[0m, in \u001b[0;36mLamini.generate\u001b[0;34m(self, prompt, model_name, output_type, max_tokens, max_new_tokens, callback, metadata)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prompt) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     64\u001b[0m     req_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_llm_req_map(\n\u001b[1;32m     65\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m     66\u001b[0m         model_name\u001b[38;5;241m=\u001b[39mmodel_name \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m         max_new_tokens\u001b[38;5;241m=\u001b[39mmax_new_tokens,\n\u001b[1;32m     70\u001b[0m     )\n\u001b[0;32m---> 71\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prompt) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/lamini/api/utils/completion.py:15\u001b[0m, in \u001b[0;36mCompletion.generate\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, params):\n\u001b[0;32m---> 15\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mmake_web_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_prefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompletions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/lamini/api/rest_requests.py:183\u001b[0m, in \u001b[0;36mmake_web_request\u001b[0;34m(key, url, http_method, json)\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m description \u001b[38;5;241m==\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetail\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[1;32m    182\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m APIError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m500 Internal Server Error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 183\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m APIError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI error \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdescription\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mjson()\n",
      "\u001b[0;31mAPIError\u001b[0m: API error {'detail': \"error_id: 243549526076307102879929981439376352577: Downloading the 'Intel/neural-chat-7b-v3-1' model. Please try again in a few minutes.\"}"
     ]
    }
   ],
   "source": [
    "## Validate setup with a named model from Hugging Face \n",
    "## By default the free-tier user has support for these base models (identified in error message)\n",
    "'''\n",
    "'hf-internal-testing/tiny-random-gpt2', \n",
    "'EleutherAI/pythia-70m', 'EleutherAI/pythia-70m-deduped', 'EleutherAI/pythia-70m-v0', \n",
    "'EleutherAI/pythia-70m-deduped-v0', 'EleutherAI/neox-ckpt-pythia-70m-deduped-v0', 'EleutherAI/neox-ckpt-pythia-70m-v1', \n",
    "'EleutherAI/neox-ckpt-pythia-70m-deduped-v1', 'EleutherAI/gpt-neo-125m', 'EleutherAI/pythia-160m', \n",
    "'EleutherAI/pythia-160m-deduped', 'EleutherAI/pythia-160m-deduped-v0', 'EleutherAI/neox-ckpt-pythia-70m', \n",
    "'EleutherAI/neox-ckpt-pythia-160m', 'EleutherAI/neox-ckpt-pythia-160m-deduped-v1', 'EleutherAI/pythia-2.8b', \n",
    "'EleutherAI/pythia-410m', 'EleutherAI/pythia-410m-v0', 'EleutherAI/pythia-410m-deduped', \n",
    "'EleutherAI/pythia-410m-deduped-v0', 'EleutherAI/neox-ckpt-pythia-410m', 'EleutherAI/neox-ckpt-pythia-410m-deduped-v1', \n",
    "'cerebras/Cerebras-GPT-111M', 'cerebras/Cerebras-GPT-256M', 'meta-llama/Llama-2-7b-hf', \n",
    "'meta-llama/Llama-2-7b-chat-hf', 'meta-llama/Llama-2-13b-chat-hf', 'meta-llama/Llama-2-70b-chat-hf', \n",
    "'Intel/neural-chat-7b-v3-1', 'mistralai/Mistral-7B-Instruct-v0.1', 'microsoft/phi-2'\n",
    "'''\n",
    "\n",
    "## Option 1: Use a named model to get an endpoint for requests\n",
    "## Models may not be pre-loaded in HF inference service - you will then see this error, so retry:\n",
    "##     Downloading the 'cerebras/Cerebras-GPT-111M' model. \n",
    "##     Please try again in a few minutes.\n",
    "llm = lamini.Lamini(\"cerebras/Cerebras-GPT-111M\")\n",
    "print(llm.generate(\"How to convert inches to centimeters? Answer in 2 sentences\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15:03:29:20,700 INFO     [lamini.py:33] Using 3.10 InferenceQueue Interface\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' To convert inches to centimeters, you can multiply the number of inches by 2.54. For example, 1 inch is equal to 2.54 centimeters.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Option 2: Use pre-defined Mistral runner\n",
    "llm = lamini.MistralRunner()\n",
    "llm(\"How to convert inches to centimeters? Answer in 2 sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15:03:29:35,899 INFO     [lamini.py:33] Using 3.10 InferenceQueue Interface\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  Of course! To convert inches to centimeters, you can use the following conversion factor: 1 inch = 2.54 centimeters. Therefore, if you want to convert a measurement in inches to centimeters, you can simply multiply it by 2.54.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Option 3: Use pre-defined LLama-2 runner\n",
    "llama = lamini.LlamaV2Runner()\n",
    "llama(\"How to convert inches to centimeters? Answer in 2 sentences\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
