{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 400.9 | Fine Tuning Large Language Models\n",
    "\n",
    " **This notebook is for my personal use only** - all sources are cited. If you are following the same learning journey please reference original sources instead. Key Resources used include:\n",
    "1.  [Fine Tuning Large Language Models](https://www.deeplearning.ai/short-courses/finetuning-large-language-models/), _DeepLearning.AI_ (2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Learn: Concepts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Fine Tuning?\n",
    "\n",
    "Process of turning a general-purpose pre-trained model into a speciliazed version suited for a particular task. Analogy: a general practitioner (GP) vs. a specialist (cardiologist).\n",
    "\n",
    "Both fine-tuning and prompt-engineering are techniques to improve the quality of a model's response to a user request but differ in cost and context:\n",
    " - Prompt engineering is easier to implement, has less upfront cost\n",
    " - Prompt engineering has data limitations (fewer examples), more hallucinations\n",
    " - Fine-tuning is more effective but has upfront compute & data processing costs\n",
    " - Fine-tuning requires high-quality data and more expertise in model training\n",
    "\n",
    "### Why Fine Tune?\n",
    "1. More cost-effective - (per-request) frees up space used by examples, context\n",
    "1. More consistent outputs - understands app requirements, response formats\n",
    "1. Reduce hallucinations - grounded in relevant data, critical for enterprise\n",
    "1. Improve data privacy - reduce breaches, data leakage in training\n",
    "1. Better performance - reliability, lower latency, better moderation options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Apply: Tasks\n",
    "\n",
    "> The following exercises should help walk through the entire process of fine-tuning a large language model using a specific provider and model endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### T1: Setup Dev Environment\n",
    "\n",
    "To explore ideas in practice, we need access to a relevant Large Language Model (LLM) and provider-hosted endpoint (API). Use the [LLM Setup](./400-00-aoia-intro.ipynb) notebook to configure environment variables and validate setup for supported providers including:\n",
    " - Open AI\n",
    " - Azure Open AI\n",
    " - Hugging Face\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### T2: Lamini Example\n",
    "\n",
    "The example from the DeepLearning.AI course uses the following libraries:\n",
    "- PyTorch (Meta) - lowest level\n",
    "- Transformers (Hugging Face) - abstracts PyTorch for easier use\n",
    "- Llama (Lamini) - abstracts working with LLama models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the lamini package\n",
    "import lamini\n",
    "import os\n",
    "lamini.api_key = os.getenv(\"LAMINI_API_KEY\")\n",
    "\n",
    "# Test the installation\n",
    "from llama import BasicModelRunner\n",
    "non_ft_model = BasicModelRunner(\"meta-llama/LLama-3-7b-hf\")\n",
    "print(non_ft_model(\"Oh say can you see\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
