{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 400.9 | Fine Tuning Large Language Models\n",
    "\n",
    " **This notebook is for my personal use only** - all sources are cited. If you are following the same learning journey please reference original sources instead. Key Resources used include:\n",
    "1.  [Fine Tuning Large Language Models](https://www.deeplearning.ai/short-courses/finetuning-large-language-models/), _DeepLearning.AI_ (2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Learn: Concepts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Fine Tuning?\n",
    "\n",
    "Process of turning a general-purpose pre-trained model into a speciliazed version suited for a particular task. Analogy: a general practitioner (GP) vs. a specialist (cardiologist).\n",
    "\n",
    "Both fine-tuning and prompt-engineering are techniques to improve the quality of a model's response to a user request but differ in cost and context:\n",
    " - Prompt engineering is easier to implement, has less upfront cost\n",
    " - Prompt engineering has data limitations (fewer examples), more hallucinations\n",
    " - Fine-tuning is more effective but has upfront compute & data processing costs\n",
    " - Fine-tuning requires high-quality data and more expertise in model training\n",
    "\n",
    "### Why Fine Tune?\n",
    "1. More cost-effective - (per-request) frees up space used by examples, context\n",
    "1. More consistent outputs - understands app requirements, response formats\n",
    "1. Reduce hallucinations - grounded in relevant data, critical for enterprise\n",
    "1. Improve data privacy - reduce breaches, data leakage in training\n",
    "1. Better performance - reliability, lower latency, better moderation options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Apply: Tasks\n",
    "\n",
    "> The following exercises should help walk through the entire process of fine-tuning a large language model using a specific provider and model endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### T1: Setup Dev Env\n",
    "\n",
    "To explore ideas in practice, we need access to a relevant Large Language Model (LLM) and provider-hosted endpoint (API). Use the [LLM Setup](./400-00-aoia-intro.ipynb) notebook to configure environment variables and validate setup for supported providers including:\n",
    " - Open AI\n",
    " - Azure Open AI\n",
    " - Hugging Face\n",
    " - Lamini (specialized use, free tier has [200 requests total](https://app.lamini.ai/account))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T2: Lamini Example\n",
    "\n",
    "The example from the DeepLearning.AI course uses the following libraries:\n",
    "- PyTorch (Meta) - lowest level\n",
    "- Transformers (Hugging Face) - abstracts PyTorch for easier use\n",
    "- Llama (Lamini) - abstracts working with LLama models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the lamini package\n",
    "# !pip install --upgrade lamini\n",
    "\n",
    "## Configure the API key\n",
    "import lamini\n",
    "import os\n",
    "lamini.api_key = os.getenv(\"LAMINI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Validate setup with simple text generation test\n",
    "llm = lamini.Lamini(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "print(llm.generate(\"How are you?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15:02:53:32,572 INFO     [lamini.py:33] Using 3.10 InferenceQueue Interface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ARGH matey! *adjusts eye patch* I be feelin' grand, thank ye for askin'! The sea be callin' me name, and me heart be yearnin' fer the next great adventure. *winks* What be bringin' ye to these fair waters? Maybe ye be lookin' fer a bit o' treasure or a swashbucklin' good time? *grin*\n"
     ]
    }
   ],
   "source": [
    "## LLamaV2Runner and MistralRunner are pre-defined - use this simpler syntax\n",
    "pirate_llm = lamini.LlamaV2Runner(system_prompt=\"You are a pirate. Say arg matey!\")\n",
    "print(pirate_llm(\"How are you?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
